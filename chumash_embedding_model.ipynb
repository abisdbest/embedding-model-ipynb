{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mishnah-Powered AI Chatbot with Gemini on Kaggle\n",
    "\n",
    "Following the model of the Torah-powered chatbot, this notebook creates a Retrieval-Augmented Generation (RAG) system that answers questions using the text of the Mishnah. It fetches all 63 tractates from the Sefaria API, indexes them using Google's Gemini models, and provides a user-friendly chat interface with Gradio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "We will install the latest LlamaIndex packages for Google GenAI integration, along with Gradio for the UI and other utility libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U llama-index llama-index-llms-google-genai llama-index-embeddings-google-genai gradio requests pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Google API Key\n",
    "\n",
    "We securely access your Google API Key from Kaggle Secrets. Ensure you have saved your key with the name `GOOGLE_API_KEY` in the **Add-ons > Secrets** menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "    print(\"API Key loaded successfully from Kaggle Secrets.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading API key: {e}. Please ensure you have set the GOOGLE_API_KEY secret.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download the Mishnah from Sefaria\n",
    "\n",
    "This section defines a `MishnahFetcher` class to download the text of all 63 tractates of the Mishnah. The Sefaria API uses a `Mishnah_{Tractate}` format for its endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "class MishnahFetcher:\n",
    "    def __init__(self):\n",
    "        # A list of all 63 tractates of the Mishnah\n",
    "        self.mishnah_tractates = [\n",
    "            'Berakhot', 'Peah', 'Demai', 'Kilayim', 'Sheviit', 'Terumot', 'Maasrot', 'Maaser Sheni', 'Challah', 'Orlah', 'Bikkurim',\n",
    "            'Shabbat', 'Eruvin', 'Pesachim', 'Shekalim', 'Yoma', 'Sukkah', 'Beitzah', 'Rosh Hashanah', 'Taanit', 'Megillah', 'Moed Katan', 'Chagigah',\n",
    "            'Yevamot', 'Ketubot', 'Nedarim', 'Nazir', 'Sotah', 'Gittin', 'Kiddushin',\n",
    "            'Bava Kamma', 'Bava Metzia', 'Bava Batra', 'Sanhedrin', 'Makkot', 'Shevuot', 'Eduyot', 'Avodah Zarah', 'Avot', 'Horayot',\n",
    "            'Zevachim', 'Menachot', 'Chullin', 'Bekhorot', 'Arakhin', 'Temurah', 'Keritot', 'Meilah', 'Tamid', 'Middot', 'Kinnim',\n",
    "            'Kelim', 'Oholot', 'Negaim', 'Parah', 'Tohorot', 'Mikvaot', 'Niddah', 'Makhshirin', 'Zavim', 'Tevul Yom', 'Yadayim', 'Oktzin'\n",
    "        ]\n",
    "\n",
    "    def get_mishnah_tractate_text(self, tractate_name):\n",
    "        # The API endpoint format is, e.g., 'Mishnah_Berakhot'\n",
    "        api_tractate_name = f\"Mishnah_{tractate_name}\"\n",
    "        url = f'https://www.sefaria.org/api/texts/{api_tractate_name}'\n",
    "        print(f\"Fetching {tractate_name}...\")\n",
    "        try:\n",
    "            response = requests.get(url, params={'context': 0, 'commentary': 0, 'pad': 0, 'lang': 'en'})\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            return data.get('text', [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Could not fetch {tractate_name}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_all_mishnah_texts(self):\n",
    "        all_texts = {}\n",
    "        for tractate in self.mishnah_tractates:\n",
    "            all_texts[tractate] = self.get_mishnah_tractate_text(tractate)\n",
    "        return all_texts\n",
    "\n",
    "# Fetch and process the text into a DataFrame\n",
    "fetcher = MishnahFetcher()\n",
    "texts = fetcher.get_all_mishnah_texts()\n",
    "\n",
    "rows = []\n",
    "for tractate, chapters in texts.items():\n",
    "    if not chapters or not isinstance(chapters, list):\n",
    "        print(f\"Skipping {tractate} due to unexpected data format.\")\n",
    "        continue\n",
    "    for chapter_idx, chapter in enumerate(chapters, 1):\n",
    "        for mishnah_idx, mishnah in enumerate(chapter, 1):\n",
    "            rows.append((tractate, chapter_idx, mishnah_idx, mishnah))\n",
    "\n",
    "df_mishnah = pd.DataFrame(rows, columns=['tractate', 'chapter', 'mishnah', 'text'])\n",
    "# Clean up HTML tags and remove empty rows\n",
    "df_mishnah['text'] = df_mishnah['text'].astype(str).str.split(\"<\").str[0]\n",
    "df_mishnah.dropna(inplace=True)\n",
    "\n",
    "print(\"\\nMishnah text successfully downloaded and processed.\")\n",
    "df_mishnah.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Google GenAI LLM and Embedding Models\n",
    "\n",
    "We initialize the models from the `google-genai` packages, which provide the latest integration with Google's services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "embedding_model = GoogleGenAIEmbedding()\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model_name=\"models/gemini-pro\",\n",
    "    temperature=0,\n",
    ")\n",
    "print(\"Successfully initialized Google GenAI LLM and Embedding models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LlamaIndex and Prepare Documents\n",
    "\n",
    "We set the global LLM and embedding model for LlamaIndex. Then, we convert our DataFrame of mishnayot into a list of `Document` objects, grouping by chapter to ensure each document has sufficient context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "documents = []\n",
    "for (tractate, chapter), group in df_mishnah.groupby(['tractate', 'chapter']):\n",
    "    # Combine all mishnayot in a chapter into a single text block for better context\n",
    "    chapter_text = \"\\n\".join(f\"Mishnah {r['mishnah']}: {r['text']}\" for _, r in group.iterrows())\n",
    "    \n",
    "    metadata = {\n",
    "        \"tractate\": tractate,\n",
    "        \"chapter\": int(chapter)\n",
    "    }\n",
    "    documents.append(Document(text=chapter_text, metadata=metadata))\n",
    "\n",
    "print(f\"Created {len(documents)} chapter-level documents from the Mishnah.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the Vector Store Index\n",
    "\n",
    "The `VectorStoreIndex` is the core of our RAG system. It takes our documents, creates vector embeddings for them, and stores them in a way that allows for fast and accurate semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up the Chat Engine\n",
    "\n",
    "We use the `ContextChatEngine`, which is ideal for RAG applications. It retrieves relevant text based on the user's query and uses it, along with a system prompt and conversation history, to formulate a grounded answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "\n",
    "# Define a system prompt for a Mishnah scholar\n",
    "system_prompt = (\n",
    "    \"You are a Mishnah scholar assistant. Your role is to answer questions strictly based on the provided text from the Mishnah. \"\n",
    "    \"Cite the tractate, chapter, and mishnah number for your answer. The context provided will be a full chapter; you must identify the specific mishnah within it. \"\n",
    "    \"If the answer cannot be found in the provided text, you must respond with: 'I cannot find a teaching in the Mishnah to answer that.'\"\n",
    ")\n",
    "\n",
    "# Create a retriever to search the index\n",
    "retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "chat_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)\n",
    "\n",
    "# Create the chat engine\n",
    "chat_engine = ContextChatEngine.from_defaults(\n",
    "    retriever=retriever,\n",
    "    memory=chat_memory,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(\"Successfully created Mishnah ContextChatEngine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launch the Gradio Web Interface\n",
    "\n",
    "Finally, we launch the Gradio `ChatInterface`. This creates a shareable public URL where anyone can interact with your Mishnah chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_interface(message, history):\n",
    "    response = chat_engine.chat(message)\n",
    "    return str(response)\n",
    "\n",
    "gr.ChatInterface(\n",
    "    chat_interface, \n",
    "    title=\"ðŸ“– Ask the Mishnah (with Gemini)\",\n",
    "    description=\"This chatbot answers questions using the 63 tractates of the Mishnah, powered by Google Gemini.\",\n",
    "    examples=[\n",
    "        \"From when may one recite the Shema in the evening?\",\n",
    "        \"What are the four principal categories of damages?\",\n",
    "        \"Who is wealthy?\"\n",
    "    ]\n",
    ").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

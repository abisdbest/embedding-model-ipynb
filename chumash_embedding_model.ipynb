{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mishnah-Powered AI Chatbot with Gemini on Kaggle\n",
    "\n",
    "This notebook creates a Retrieval-Augmented Generation (RAG) chatbot that answers questions using the entire Mishnah (all six orders). It is an adaptation of the Torah Chatbot, modified to use a different corpus of text from the Sefaria API.\n",
    "\n",
    "It uses Google's Gemini models via the latest LlamaIndex `google-genai` integration and runs seamlessly in a Kaggle environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "We install the necessary libraries, including the latest LlamaIndex packages for Google's Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U llama-index llama-index-llms-google-genai llama-index-embeddings-google-genai gradio requests pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Google API Key using Kaggle Secrets\n",
    "\n",
    "**Instructions:**\n",
    "1. In the notebook editor, go to **Add-ons > Secrets**.\n",
    "2. Add a new secret with the name `GOOGLE_API_KEY`.\n",
    "3. Paste your API key from [Google AI Studio](https://aistudio.google.com/) as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "    print(\"API Key loaded successfully from Kaggle Secrets.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading API key: {e}. Please ensure you have set the GOOGLE_API_KEY secret correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download the Mishnah from Sefaria\n",
    "\n",
    "We define a `MishnahFetcher` class to get a list of all Mishnah tractates and then download the English text for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class MishnahFetcher:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://www.sefaria.org/api\"\n",
    "        self.mishnah_tractates = self._get_mishnah_tractates()\n",
    "\n",
    "    def _get_mishnah_tractates(self):\n",
    "        """Fetches the list of all tractates in the Mishnah from the Sefaria API."""\n",
    "        print(\"Fetching list of Mishnah tractates...\")\n",
    "        try:\n",
    "            index_res = requests.get(f\"{self.base_url}/index\")\n",
    "            index_res.raise_for_status()\n",
    "            index_data = index_res.json()\n",
    "            for category in index_data:\n",
    "                if category.get('category') == 'Mishnah':\n",
    "                    tractate_titles = [content['title'] for content in category['contents']]\n",
    "                    print(f\"Found {len(tractate_titles)} tractates.\")\n",
    "                    return tractate_titles\n",
    "            return []\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching index: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_mishnah_text(self, tractate_title, lang='en'):\n",
    "        """Fetches the full English text for a single Mishnah tractate."""\n",
    "        # Sefaria API uses underscores for spaces in titles, e.g., 'Mishnah_Berakhot'\n",
    "        api_title = f\"Mishnah {tractate_title}\".replace(\" \", \"_\")\n",
    "        url = f'{self.base_url}/texts/{api_title}'\n",
    "        try:\n",
    "            response = requests.get(url, params={'context': 0, 'commentary': 0, 'pad': 0, 'lang': lang})\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            return data.get('text', [])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Could not fetch {tractate_title}: {e}\")\n",
    "            return []\n",
    "\n",
    "# --- Main Data Fetching and Processing ---\n",
    "fetcher = MishnahFetcher()\n",
    "all_texts = {}\n",
    "for tractate in fetcher.mishnah_tractates:\n",
    "    print(f\"Downloading {tractate}...\")\n",
    "    all_texts[tractate] = fetcher.get_mishnah_text(tractate, lang='en')\n",
    "    time.sleep(1) # Be polite to the API\n",
    "\n",
    "rows = []\n",
    "for tractate, chapters in all_texts.items():\n",
    "    if not chapters:\n",
    "        continue\n",
    "    for chapter_idx, chapter in enumerate(chapters, 1):\n",
    "        for mishnah_idx, mishnah_text in enumerate(chapter, 1):\n",
    "            rows.append((tractate, chapter_idx, mishnah_idx, mishnah_text))\n",
    "\n",
    "df_mishnah = pd.DataFrame(rows, columns=['tractate', 'chapter', 'mishnah', 'text'])\n",
    "df_mishnah['text'] = df_mishnah['text'].str.split('<').str[0].str.strip()\n",
    "df_mishnah.dropna(inplace=True)\n",
    "\n",
    "print(\"\\nMishnah text successfully downloaded and processed.\")\n",
    "df_mishnah.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Google GenAI LLM and Embedding Models\n",
    "\n",
    "We instantiate the models using the correct classes from the `google-genai` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "embedding_model = GoogleGenAIEmbedding()\n",
    "\n",
    "llm = GoogleGenAI(\n",
    "    model_name=\"models/gemini-pro\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(\"Successfully initialized Google GenAI LLM and Embedding models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LlamaIndex and Prepare Documents\n",
    "\n",
    "We group the Mishnayot into chapter-level `Document` objects and configure the global `Settings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "documents = []\n",
    "for (tractate, chapter), group in df_mishnah.groupby(['tractate', 'chapter']):\n",
    "    # Combine all mishnayot in a chapter into a single text block\n",
    "    chapter_text = \"\\n\".join(f\"Mishnah {r['mishnah']}: {r['text']}\" for _, r in group.iterrows())\n",
    "    \n",
    "    metadata = {\n",
    "        \"tractate\": tractate,\n",
    "        \"chapter\": int(chapter)\n",
    "    }\n",
    "    documents.append(Document(text=chapter_text, metadata=metadata))\n",
    "\n",
    "print(f\"Created {len(documents)} chapter-level documents from the Mishnah.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the Vector Store Index\n",
    "\n",
    "Now we create the `VectorStoreIndex`. This process creates embeddings for each chapter and stores them in a vector database for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set Up the Chat Engine\n",
    "\n",
    "We use the `ContextChatEngine`, which is ideal for RAG as it directly supports a system prompt to guide the chatbot's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "\n",
    "# Update the system prompt for the Mishnah\n",
    "system_prompt = (\n",
    "    \"You are a Talmudic scholar assistant. Your role is to answer questions strictly based on the provided Mishnah texts. \"\n",
    "    \"Cite the tractate, chapter, and mishnah number for your answer. \"\n",
    "    \"If the answer cannot be found in the provided texts, you must respond with: 'I cannot find a Mishnah to answer that.'\"\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=10)\n",
    "chat_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)\n",
    "\n",
    "chat_engine = ContextChatEngine.from_defaults(\n",
    "    retriever=retriever,\n",
    "    memory=chat_memory,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(\"Successfully created ContextChatEngine for Mishnah.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launch the Gradio Web Interface\n",
    "\n",
    "Finally, we launch the Gradio `ChatInterface` with a new title, description, and examples relevant to the Mishnah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_interface(message, history):\n",
    "    response = chat_engine.chat(message)\n",
    "    return str(response)\n",
    "\n",
    "gr.ChatInterface(\n",
    "    chat_interface, \n",
    "    title=\"ðŸ“– Ask the Mishnah (with Gemini)\",\n",
    "    description=\"This chatbot answers questions using the entire Mishnah, powered by Google's Gemini.\",\n",
    "    examples=[\"From when may one recite the Shema in the evening?\", \"What are the four primary categories of damages?\", \"Who has no portion in the world to come?\"]\n",
    ").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
